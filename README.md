# SparklAI âœ¨

This is a fake social media site that generates all of its users, posts, images, and DMs with open source AI APIs like Llama and Stable Diffusion.

Front-end is a SvelteKit app, with source in `src/`, startable with `pnpm run dev`.

Do the thing first with `pnpm run db:push` to database it up.

## Docker Deployment

### Using Docker Compose (Recommended)

1. Copy `.env.docker.example` to `.env` and configure your environment variables:
   ```bash
   cp .env.docker.example .env
   ```

2. Edit `.env` to point to your LLM and Stable Diffusion services:
   - `CHAT_URL`: Your LLM API endpoint (default uses host.docker.internal to access localhost services)
   - `SD_URL`: Your Stable Diffusion API endpoint

3. Build and start the application:
   ```bash
   docker compose up -d
   ```

4. Initialize the database (first time only):
   ```bash
   docker compose exec app sh -c "pnpm run db:push"
   ```

5. Access the application at http://localhost:3000

### Using Docker Only

1. Build the Docker image:
   ```bash
   docker build -t sparklai .
   ```

2. Run the container:
   ```bash
   docker run -d \
     -p 3000:3000 \
     -v sparklai-data:/data \
     -e DATABASE_URL="file:/data/local.db" \
     -e CHAT_URL="http://host.docker.internal:1234/v1/" \
     -e CHAT_MODEL="meta-llama-3.1-8b-instruct" \
     -e SD_URL="http://host.docker.internal:7860/sdapi/v1/" \
     --name sparklai \
     sparklai
   ```

3. Initialize the database (first time only):
   ```bash
   docker exec sparklai sh -c "pnpm run db:push"
   ```

### Notes

- The SQLite database is persisted in a Docker volume at `/data/local.db`
- `host.docker.internal` allows the container to access services running on your host machine
- If your LLM/SD services are in other containers, use their service names instead
- For production, consider using environment-specific configuration files

### Production Deployment

For production deployments, use the `docker-compose.prod.yml` file:

```bash
docker compose -f docker-compose.prod.yml up -d
```

This configuration includes:
- Environment file support (`.env`)
- Health checks
- Security options (no-new-privileges)
- Resource limits (CPU and memory)
- Automatic restart policy

Make sure to create a `.env` file with your production configuration before deploying.

## Future

There are a few things that would really improve the realism/accuracy of the content generated:

- Define and inform the LLM of the relationship between specific users
- Allow human users to define their profile information, and provide it as context when generating responses
- Support multiple models and automatic switching between them based on the task
  - For example, different Stable Diffusion models depending on whether the image should be photorealistic, a drawing, an animated scene, etc.
- Allow human users to see/edit user metadata used by the LLM
